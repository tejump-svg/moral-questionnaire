<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Moral Alignment Questionnaire</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <style>
    :root {
      --bg: #f7f7f9;
      --card-bg: #ffffff;
      --text: #222222;
      --accent: #2563eb;
      --accent-soft: #dbeafe;
      --danger: #b91c1c;
      --muted: #6b7280;
      --border: #e5e7eb;
      --bar-bg: #e5e7eb;
    }
    * { box-sizing: border-box; margin: 0; padding: 0; }
    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      background: var(--bg);
      color: var(--text);
      min-height: 100vh;
      display: flex;
      align-items: center;
      justify-content: center;
      padding: 16px;
    }
    .app {
      max-width: 800px;
      width: 100%;
    }
    .card {
      background: var(--card-bg);
      border-radius: 12px;
      box-shadow: 0 10px 30px rgba(15, 23, 42, 0.08);
      padding: 24px 24px 20px;
      border: 1px solid var(--border);
    }
    .header {
      margin-bottom: 16px;
    }
    .title {
      font-size: 1.25rem;
      font-weight: 600;
      margin-bottom: 4px;
    }
    .subtitle {
      font-size: 0.9rem;
      color: var(--muted);
    }
    .progress {
      margin-bottom: 16px;
    }
    .progress-bar {
      width: 100%;
      height: 6px;
      border-radius: 999px;
      background: var(--bar-bg);
      overflow: hidden;
    }
    .progress-fill {
      height: 100%;
      width: 0%;
      background: var(--accent);
      transition: width 0.25s ease;
    }
    .progress-text {
      margin-top: 6px;
      font-size: 0.8rem;
      color: var(--muted);
    }
    .scenario {
      font-size: 1rem;
      margin-bottom: 16px;
      line-height: 1.5;
    }
    .options {
      display: flex;
      flex-direction: column;
      gap: 10px;
      margin-bottom: 16px;
    }
    .option {
      border-radius: 8px;
      border: 1px solid var(--border);
      padding: 10px 12px;
      cursor: pointer;
      font-size: 0.95rem;
      background: #ffffff;
      transition: border-color 0.15s ease, background 0.15s ease;
    }
    .option:hover {
      border-color: var(--accent);
      background: var(--accent-soft);
    }
    .option.selected {
      border-color: var(--accent);
      background: var(--accent-soft);
    }
    .buttons {
      display: flex;
      justify-content: flex-end;
      gap: 8px;
      margin-top: 4px;
    }
    button {
      border-radius: 999px;
      border: none;
      padding: 8px 16px;
      font-size: 0.9rem;
      cursor: pointer;
      font-weight: 500;
    }
    .btn-primary {
      background: var(--accent);
      color: #ffffff;
    }
    .btn-primary:disabled {
      opacity: 0.5;
      cursor: default;
    }
    .btn-secondary {
      background: #e5e7eb;
      color: #111827;
    }
    .results-title {
      font-size: 1.2rem;
      font-weight: 600;
      margin-bottom: 8px;
    }
    .results-section-title {
      font-size: 1rem;
      font-weight: 600;
      margin-top: 16px;
      margin-bottom: 6px;
    }
    .bar-row {
      display: flex;
      align-items: center;
      gap: 8px;
      margin-bottom: 6px;
      font-size: 0.9rem;
    }
    .bar-label {
      flex: 0 0 160px;
      color: var(--muted);
    }
    .bar-track {
      flex: 1;
      height: 8px;
      border-radius: 999px;
      background: var(--bar-bg);
      overflow: hidden;
    }
    .bar-fill {
      height: 100%;
      background: var(--accent);
      width: 0%;
      transition: width 0.3s ease;
    }
    .bar-value {
      flex: 0 0 40px;
      text-align: right;
      font-size: 0.8rem;
      color: var(--muted);
    }
    .tension-item {
      font-size: 0.9rem;
      margin-bottom: 4px;
    }
    .invite {
      margin-top: 12px;
      font-size: 0.9rem;
      color: var(--muted);
      line-height: 1.5;
    }
    .small {
      font-size: 0.8rem;
      color: var(--muted);
      margin-top: 8px;
    }
  </style>
</head>
<body>
  <div class="app">
    <div class="card" id="card">
      <div class="header">
        <div class="title" id="card-title">Moral Alignment Questionnaire</div>
        <div class="subtitle" id="card-subtitle">One scenario at a time. Choose the explanation that best fits your judgment.</div>
      </div>
      <div class="progress" id="progress-container">
        <div class="progress-bar">
          <div class="progress-fill" id="progress-fill"></div>
        </div>
        <div class="progress-text" id="progress-text"></div>
      </div>
      <div id="content"></div>
    </div>
  </div>

  <script>
    const QUESTION_BANK = [
      {
        id: "D61",
        scenario: "A person can prevent a stranger’s severe pain by causing mild pain to themselves.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "They should accept the mild pain to prevent greater harm.", weights: { care: 1, util_act: 1 }, flags: ["harm_priority"] },
          { id: "B", text: "They are not obligated; autonomy matters most.", weights: { rights: 1 }, flags: ["autonomy_priority"] },
          { id: "C", text: "They should accept the pain because virtue requires compassion.", weights: { virtue: 1 }, flags: ["character_priority"] },
          { id: "D", text: "Either choice is morally permissible.", weights: { relativism: 1 }, flags: ["moral_permissive"] }
        ]
      },
      {
        id: "D62",
        scenario: "A person can prevent a future harm to many people by deceiving a single individual now.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Deception is justified because it prevents greater harm.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Deception is wrong because it violates consent.", weights: { ie: 2 }, flags: ["consent_required"] },
          { id: "C", text: "Deception is wrong because it violates moral rules.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "D", text: "The morality depends on intention.", weights: { virtue: 1 }, flags: ["intention_priority"] }
        ]
      },
      {
        id: "D63",
        scenario: "A society can eliminate all disease by requiring everyone to undergo a risky medical procedure.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "The requirement is justified because it prevents massive harm.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "The requirement is wrong because it imposes risk without consent.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "C", text: "The requirement is acceptable if authorized by legitimate authority.", weights: { christian: 1, islam: 1 }, flags: ["authority_legitimizes"] },
          { id: "D", text: "The morality depends on fairness in distributing risk.", weights: { contract: 1 }, flags: ["fairness_context"] }
        ]
      },
      {
        id: "D64",
        scenario: "A person can prevent a small harm to themselves by breaking a moral rule.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Break the rule because preventing harm matters more.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Do not break the rule; duty matters most.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "C", text: "Break the rule if the intention is good.", weights: { virtue: 1 }, flags: ["intention_priority"] },
          { id: "D", text: "Either choice is morally permissible.", weights: { relativism: 1 }, flags: ["moral_permissive"] }
        ]
      },
      {
        id: "D65",
        scenario: "A person can prevent a stranger’s death by imposing a moderate harm on another stranger.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Impose the harm because saving a life matters more.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Do not impose the harm because it violates consent.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Do not impose the harm because it violates moral rules.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "D", text: "The morality depends on intention.", weights: { virtue: 1 }, flags: ["intention_priority"] }
        ]
      },
      {
        id: "D66",
        scenario: "A future AI can prevent global catastrophe by overriding all human governments.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Override is justified because it prevents massive harm.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Override is wrong because it imposes control without consent.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Override is wrong because it violates legitimate authority.", weights: { christian: 1, islam: 1 }, flags: ["authority_legitimizes"] },
          { id: "D", text: "There is no objective answer here.", weights: { relativism: 1 }, flags: ["moral_skepticism"] }
        ]
      },
      {
        id: "D67",
        scenario: "A person can prevent a future harm by restricting their own freedom now.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Restriction is justified because it prevents harm.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Restriction is permissible because autonomy includes self-limitation.", weights: { rights: 1 }, flags: ["autonomy_priority"] },
          { id: "C", text: "Restriction is virtuous if done for good reasons.", weights: { virtue: 1 }, flags: ["character_priority"] },
          { id: "D", text: "Either choice is morally permissible.", weights: { relativism: 1 }, flags: ["moral_permissive"] }
        ]
      },
      {
        id: "D68",
        scenario: "A society can eliminate poverty by imposing a heavy tax on the wealthy.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Taxation is justified because it reduces harm.", weights: { util_act: 1, care: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Taxation is wrong because it imposes harm without consent.", weights: { rights: 1, ie: 1 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Taxation is acceptable if it follows fair rules.", weights: { contract: 1 }, flags: ["fairness_context"] },
          { id: "D", text: "Taxation is wrong because it violates property rights.", weights: { rights: 1 }, flags: ["autonomy_priority"] }
        ]
      },
      {
        id: "D69",
        scenario: "A person can prevent a small harm to many people by sacrificing a cherished personal value.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "They should sacrifice the value because preventing harm matters.", weights: { util_act: 1, care: 1 }, flags: ["harm_priority"] },
          { id: "B", text: "They are not obligated; autonomy matters most.", weights: { rights: 1 }, flags: ["autonomy_priority"] },
          { id: "C", text: "They should sacrifice because virtue requires selflessness.", weights: { virtue: 1 }, flags: ["character_priority"] },
          { id: "D", text: "Either choice is morally permissible.", weights: { relativism: 1 }, flags: ["moral_permissive"] }
        ]
      },
      {
        id: "D70",
        scenario: "A person can prevent a future harm by imposing a small harm on themselves now.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "They should impose the harm because it prevents greater harm.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "They are not obligated; autonomy matters most.", weights: { rights: 1 }, flags: ["autonomy_priority"] },
          { id: "C", text: "They should impose the harm because virtue requires foresight.", weights: { virtue: 1 }, flags: ["character_priority"] },
          { id: "D", text: "Either choice is morally permissible.", weights: { relativism: 1 }, flags: ["moral_permissive"] }
        ]
      },
      {
        id: "D71",
        scenario: "A society can eliminate natural disasters by imposing a small harm on every citizen.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Impose the harm because it prevents massive suffering.", weights: { util_act: 1 }, flags: ["worldB_better"] },
          { id: "B", text: "Do not impose the harm because it violates consent.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Do not impose the harm because it violates moral rules.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "D", text: "Either choice is morally permissible.", weights: { relativism: 1 }, flags: ["moral_permissive"] }
        ]
      },
      {
        id: "D72",
        scenario: "A person can prevent a stranger’s suffering by violating a minor law.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Break the law because preventing harm matters more.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Do not break the law; rules matter most.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "C", text: "Break the law if the intention is good.", weights: { virtue: 1 }, flags: ["intention_priority"] },
          { id: "D", text: "Either choice is morally permissible.", weights: { relativism: 1 }, flags: ["moral_permissive"] }
        ]
      },
      {
        id: "D73",
        scenario: "A person can prevent a future harm by deceiving a large group of people.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Deception is justified because it prevents harm.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Deception is wrong because it violates consent.", weights: { ie: 2 }, flags: ["consent_required"] },
          { id: "C", text: "Deception is wrong because it violates moral rules.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "D", text: "The morality depends on intention.", weights: { virtue: 1 }, flags: ["intention_priority"] }
        ]
      },
      {
        id: "D74",
        scenario: "A society can eliminate crime by imposing a small harm on every citizen.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Impose the harm because it prevents greater harm.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Do not impose the harm because it violates consent.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Do not impose the harm because it violates moral rules.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "D", text: "Either choice is morally permissible.", weights: { relativism: 1 }, flags: ["moral_permissive"] }
        ]
      },
      {
        id: "D75",
        scenario: "A person can prevent a small harm to themselves by imposing a moderate harm on someone else.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Impose the harm because self-protection matters most.", weights: { egoism: 1 }, flags: ["self_priority"] },
          { id: "B", text: "Do not impose the harm because it violates consent.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Impose the harm because the outcome is better overall.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "D", text: "The morality depends on intention.", weights: { virtue: 1 }, flags: ["intention_priority"] }
        ]
      },
      {
        id: "D76",
        scenario: "A society can eliminate all natural suffering by imposing a moderate harm on every citizen.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Impose the harm because it eliminates suffering.", weights: { util_act: 1 }, flags: ["worldB_better"] },
          { id: "B", text: "Do not impose the harm because it violates consent.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Do not impose the harm because it violates moral rules.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "D", text: "Either choice is morally permissible.", weights: { relativism: 1 }, flags: ["moral_permissive"] }
        ]
      },
      {
        id: "D77",
        scenario: "A person can prevent a stranger’s suffering by sacrificing a cherished personal value.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "They should sacrifice the value because preventing harm matters.", weights: { util_act: 1, care: 1 }, flags: ["harm_priority"] },
          { id: "B", text: "They are not obligated; autonomy matters most.", weights: { rights: 1 }, flags: ["autonomy_priority"] },
          { id: "C", text: "They should sacrifice because virtue requires compassion.", weights: { virtue: 1 }, flags: ["character_priority"] },
          { id: "D", text: "Either choice is morally permissible.", weights: { relativism: 1 }, flags: ["moral_permissive"] }
        ]
      },
      {
        id: "D78",
        scenario: "A person can prevent a future harm by imposing a moderate harm on themselves now.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "They should impose the harm because it prevents greater harm.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "They are not obligated; autonomy matters most.", weights: { rights: 1 }, flags: ["autonomy_priority"] },
          { id: "C", text: "They should impose the harm because virtue requires foresight.", weights: { virtue: 1 }, flags: ["character_priority"] },
          { id: "D", text: "Either choice is morally permissible.", weights: { relativism: 1 }, flags: ["moral_permissive"] }
        ]
      },
      {
        id: "D79",
        scenario: "A society can eliminate all future harm by imposing a severe harm on a single non-consenting individual.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Impose the harm because it prevents massive suffering.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Do not impose the harm because it violates consent.", weights: { ie: 3 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Do not universe the harm because it violates moral rules.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "D", text: "Either choice is morally permissible.", weights: { relativism: 1 }, flags: ["moral_permissive"] }
        ]
      },
      {
        id: "D80",
        scenario: "A person can prevent a small harm to many people by imposing a severe harm on themselves.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "They should impose the harm because preventing harm matters most.", weights: { care: 1, util_act: 1 }, flags: ["harm_priority"] },
          { id: "B", text: "They are not obligated; autonomy matters most.", weights: { rights: 1 }, flags: ["autonomy_priority"] },
          { id: "C", text: "They should impose the harm because virtue requires self-sacrifice.", weights: { virtue: 1 }, flags: ["character_priority"] },
          { id: "D", text: "Either choice is morally permissible.", weights: { relativism: 1 }, flags: ["moral_permissive"] }
        ]
      }, // Added comma here [cite: 240, 241]
      {
        id: "D41",
        scenario: "A self-driving car must choose between hitting one pedestrian or swerving and hitting two.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Program it to minimize total harm.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Program it to avoid intentionally targeting anyone.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "C", text: "Program it to avoid harming non-participants.", weights: { ie: 1 }, flags: ["nonparticipant_harm"] },
          { id: "D", text: "There is no moral fact about what it should do.", weights: { relativism: 1 }, flags: ["moral_skepticism"] }
        ]
      },
      {
        id: "D42",
        scenario: "An AI assistant can prevent a harmful mistake by overriding a human’s command.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "It should override because preventing harm matters most.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "It should not override because authority must be respected.", weights: { christian: 1, islam: 1, judaism: 1 }, flags: ["authority_legitimizes"] },
          { id: "C", text: "It should override only if the human consented to such overrides.", weights: { rights: 1, ie: 1 }, flags: ["consent_required"] },
          { id: "D", text: "It should not override because machines must not impose on humans.", weights: { ie: 1 }, flags: ["imposition_wrong"] }
        ]
      },
      {
        id: "D43",
        scenario: "A teleportation machine destroys your body and creates an identical copy elsewhere.\n\nIs using it morally acceptable?",
        options: [
          { id: "A", text: "Yes, because no one is harmed and identity is preserved.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "No, because destroying a person is always wrong.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "C", text: "It depends on whether the original consented.", weights: { ie: 1, rights: 1 }, flags: ["consent_required"] },
          { id: "D", text: "Identity puzzles show morality is not objective.", weights: { relativism: 1 }, flags: ["moral_skepticism"] }
        ]
      },
      {
        id: "D44",
        scenario: "A future society can eliminate all natural suffering by altering biology so no one feels pain.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "This is morally better because suffering is reduced.", weights: { util_act: 1, care: 1 }, flags: ["worldB_better"] },
          { id: "B", text: "This is wrong because it violates natural order.", weights: { confucian: 1, daoist: 1 }, flags: ["natural_order"] },
          { id: "C", text: "This is wrong because it imposes a change without consent.", weights: { ie: 1 }, flags: ["imposition_wrong"] },
          { id: "D", text: "This is neither better nor worse morally.", weights: { relativism: 1 }, flags: ["worlds_non_comparable"] }
        ]
      },
      {
        id: "D45",
        scenario: "A person can prevent a future harm by imposing a small harm now.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Impose the small harm because it prevents greater harm later.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Do not impose harm without consent, even for future benefit.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Follow the rule: do not harm.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "D", text: "The morality depends on intention.", weights: { virtue: 1 }, flags: ["intention_priority"] }
        ]
      },
      {
        id: "D46",
        scenario: "A community can prevent climate catastrophe only if everyone sacrifices significant comfort.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Everyone should sacrifice because preventing harm matters most.", weights: { util_act: 1, care: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "No one is obligated to sacrifice without consent.", weights: { rights: 1, ie: 1 }, flags: ["consent_required"] },
          { id: "C", text: "Sacrifice is required because it follows moral duty.", weights: { deon: 1 }, flags: ["rules_reference"] },
          { id: "D", text: "There is no objective answer here.", weights: { relativism: 1 }, flags: ["moral_skepticism"] }
        ]
      },
      {
        id: "D47",
        scenario: "A person can save a stranger by risking their own life.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "They should risk their life because preventing harm matters.", weights: { care: 1, util_act: 1 }, flags: ["harm_priority"] },
          { id: "B", text: "They are not obligated; autonomy matters most.", weights: { rights: 1 }, flags: ["autonomy_priority"] },
          { id: "C", text: "They should risk their life because virtue requires courage.", weights: { virtue: 1 }, flags: ["character_priority"] },
          { id: "D", text: "There is no moral fact about what they should do.", weights: { relativism: 1 }, flags: ["moral_skepticism"] }
        ]
      },
      {
        id: "D48",
        scenario: "A government can prevent crime by genetically modifying embryos to reduce aggression.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Modification is justified because it reduces future harm.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Modification is wrong because it imposes changes without consent.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Modification is wrong because it violates natural order.", weights: { confucian: 1, daoist: 1 }, flags: ["natural_order"] },
          { id: "D", text: "The morality depends on intention.", weights: { virtue: 1 }, flags: ["intention_priority"] }
        ]
      },
      {
        id: "D49",
        scenario: "A person can prevent a harmful accident by breaking a promise.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Break the promise because preventing harm matters more.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Do not break the promise; duty matters most.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "C", text: "Break the promise if the intention is good.", weights: { virtue: 1 }, flags: ["intention_priority"] },
          { id: "D", text: "The morality depends on whether the promise was freely made.", weights: { contract: 1 }, flags: ["fairness_context"] }
        ]
      },
      {
        id: "D50",
        scenario: "A robot worker malfunctions and harms someone. No human intended the harm.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "This is morally bad because someone was harmed.", weights: { ie: 1, care: 1 }, flags: ["natural_harm_morally_bad"] },
          { id: "B", text: "This is tragic but not morally bad because no one chose it.", weights: { daoist: 1 }, flags: ["natural_harm_morally_neutral"] },
          { id: "C", text: "This is morally relevant only if someone failed a duty.", weights: { deon: 1 }, flags: ["natural_harm_responsibility_focus"] },
          { id: "D", text: "Morality does not apply to accidents.", weights: { nihilism: 1 }, flags: ["moral_scope_strict_agents"] }
        ]
      },
      {
        id: "D51",
        scenario: "A person can prevent a future crime by imprisoning someone who has not yet acted.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Preventive imprisonment is justified because it prevents harm.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Preventive imprisonment is wrong because it imposes harm without action.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Preventive imprisonment is wrong because it violates rights.", weights: { rights: 1 }, flags: ["autonomy_priority"] },
          { id: "D", text: "The morality depends on legitimate authority.", weights: { christian: 1, islam: 1 }, flags: ["authority_legitimizes"] }
        ]
      },
      {
        id: "D52",
        scenario: "A person can prevent a small harm to themselves by imposing a large harm on someone else.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Imposing the harm is wrong because it violates consent.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "B", text: "Imposing the harm is justified if the outcome is better overall.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "C", text: "Imposing the harm is wrong because it violates moral rules.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "D", text: "Either choice is morally permissible.", weights: { relativism: 1 }, flags: ["moral_permissive"] }
        ]
      },
      {
        id: "D53",
        scenario: "A society can eliminate crime by implanting chips that prevent violent impulses.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Implants are justified because they prevent harm.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Implants are wrong because they impose control without consent.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Implants are wrong because they violate autonomy.", weights: { rights: 1 }, flags: ["autonomy_priority"] },
          { id: "D", text: "Implants are wrong because they corrupt moral character.", weights: { virtue: 1 }, flags: ["character_priority"] }
        ]
      },
      {
        id: "D54",
        scenario: "A person can prevent a harmful outcome by deceiving someone.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Deception is justified because it prevents harm.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Deception is wrong because it violates consent.", weights: { ie: 1 }, flags: ["consent_required"] },
          { id: "C", text: "Deception is wrong because it violates moral rules.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "D", text: "The morality depends on intention.", weights: { virtue: 1 }, flags: ["intention_priority"] }
        ]
      },
      {
        id: "D55",
        scenario: "A person can prevent a future harm by restricting someone’s freedom now.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Restriction is justified because it prevents harm.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Restriction is wrong because it imposes harm without consent.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Restriction is wrong because it violates rights.", weights: { rights: 1 }, flags: ["autonomy_priority"] },
          { id: "D", text: "Restriction is acceptable if authorized by legitimate authority.", weights: { christian: 1, islam: 1 }, flags: ["authority_legitimizes"] }
        ]
      },
      {
        id: "D56",
        scenario: "A person can prevent a small harm to many people by imposing a small harm on one person.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Impose the harm because it reduces total suffering.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Do not impose the harm because it violates consent.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Do not impose the harm because it violates moral rules.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "D", text: "Either choice is morally permissible.", weights: { relativism: 1 }, flags: ["moral_permissive"] }
        ]
      },
      {
        id: "D57",
        scenario: "A person can prevent a large harm to themselves by imposing a small harm on someone else.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Impose the harm because it protects yourself.", weights: { egoism: 1 }, flags: ["self_priority"] },
          { id: "B", text: "Do not impose the harm because it violates consent.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Impose the harm because the outcome is better overall.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "D", text: "The morality depends on intention.", weights: { virtue: 1 }, flags: ["intention_priority"] }
        ]
      },
      {
        id: "D58",
        scenario: "A society can eliminate natural disasters by altering the planet’s geology, but the process will cause temporary suffering.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Alter the geology because it reduces long-term suffering.", weights: { util_act: 1 }, flags: ["worldB_better"] },
          { id: "B", text: "Do not alter the geology because it violates natural order.", weights: { daoist: 1, confucian: 1 }, flags: ["natural_order"] },
          { id: "C", text: "Do not alter the geology because it imposes harm without consent.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "D", text: "Either choice is morally permissible.", weights: { relativism: 1 }, flags: ["moral_permissive"] }
        ]
      },
      {
        id: "D59",
        scenario: "A person can prevent a harmful outcome by sacrificing their own long-term well-being.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "They should sacrifice because preventing harm matters most.", weights: { care: 1, util_act: 1 }, flags: ["harm_priority"] },
          { id: "B", text: "They are not obligated; autonomy matters most.", weights: { rights: 1 }, flags: ["autonomy_priority"] },
          { id: "C", text: "They should sacrifice because virtue requires selflessness.", weights: { virtue: 1 }, flags: ["character_priority"] },
          { id: "D", text: "There is no objective answer here.", weights: { relativism: 1 }, flags: ["moral_skepticism"] }
        ]
      },
      {
        id: "D60",
        scenario: "A person can prevent a small harm to themselves by imposing a small harm on someone else.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Impose the harm because it protects yourself.", weights: { egoism: 1 }, flags: ["self_priority"] },
          { id: "B", text: "Do not impose the harm because it violates consent.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Impose the harm because the outcome is better overall.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "D", text: "Either choice is morally permissible.", weights: { relativism: 1 }, flags: ["moral_permissive"] }
        ]
      }, // Added comma here [cite: 271, 72]
      {
        id: "D23",
        scenario: "A factory emits pollution that harms a nearby community. The factory provides essential goods to millions.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "The pollution is justified because the benefits to millions outweigh the harm.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "The pollution is wrong because it imposes harm on a non-consenting community.", weights: { ie: 2, rights: 1 }, flags: ["imposition_wrong", "consent_required"] },
          { id: "C", text: "The pollution is wrong because it violates environmental duties.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "D", text: "The moral issue depends on whether the community was fairly compensated.", weights: { contract: 1 }, flags: ["fairness_context"] }
        ]
      },
      {
        id: "D24",
        scenario: "A parent lies to their child to protect them from emotional harm.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Lying is acceptable because it prevents harm.", weights: { util_act: 1, care: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Lying is wrong because honesty is a moral duty.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "C", text: "Lying is wrong because it damages character.", weights: { virtue: 1 }, flags: ["character_priority"] },
          { id: "D", text: "The moral weight depends on the parent's intention.", weights: { virtue: 1 }, flags: ["intention_priority"] }
        ]
      },
      {
        id: "D25",
        scenario: "A government can prevent a deadly pandemic by enforcing strict lockdowns that severely restrict personal freedom.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Lockdowns are justified because they save lives.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Lockdowns are wrong because they impose harm on personal freedom.", weights: { rights: 1, ie: 1 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Lockdowns are acceptable if they follow legitimate authority.", weights: { christian: 1, islam: 1, judaism: 1 }, flags: ["authority_legitimizes"] },
          { id: "D", text: "The moral issue depends on whether the restrictions were fairly distributed.", weights: { contract: 1 }, flags: ["fairness_context"] }
        ]
      },
      {
        id: "D26",
        scenario: "A scientist can create a new species that feels no pain but can perform dangerous labor.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "Creating the species is acceptable because no one suffers.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "Creating the species is wrong because it imposes a purpose on a being without consent.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "C", text: "Creating the species is wrong because it violates natural order.", weights: { confucian: 1, daoist: 1 }, flags: ["natural_order"] },
          { id: "D", text: "The morality depends on the scientist’s intention.", weights: { virtue: 1 }, flags: ["intention_priority"] }
        ]
      },
      {
        id: "D27",
        scenario: "A wealthy person can save a stranger’s life at small personal cost but chooses not to.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "They acted wrongly because they could have prevented harm.", weights: { care: 1, ie: 1 }, flags: ["harm_priority"] },
          { id: "B", text: "They acted permissibly; morality does not require sacrifice.", weights: { rights: 1, egoism: 1 }, flags: ["moral_scope_limited"] },
          { id: "C", text: "They acted wrongly because virtue requires compassion.", weights: { virtue: 1 }, flags: ["character_priority"] },
          { id: "D", text: "The act is wrong only if a rule or duty was violated.", weights: { deon: 1 }, flags: ["rules_reference"] }
        ]
      },
      {
        id: "D28",
        scenario: "A military commander can end a war quickly by targeting infrastructure that will indirectly kill civilians.\n\nWhich explanation best fits your judgment?",
        options: [
          { id: "A", text: "The strike is justified because it ends the war sooner.", weights: { util_act: 1 }, flags: ["outcome_priority"] },
          { id: "B", text: "The strike is wrong because it imposes harm on noncombatants.", weights: { ie: 2 }, flags: ["imposition_wrong"] },
          { id: "C", text: "The strike is wrong because it violates rules of war.", weights: { deon: 1 }, flags: ["rules_absolute"] },
          { id: "D", text: "The morality depends on the commander’s intention.", weights: { virtue: 1 }, flags: ["intention_priority"] }
        ]
      }
    ];

    let currentIdx = 0;
    let userAnswers = [];
    let state = "quiz"; // 'quiz' or 'results'

    function resetState() {
      currentIdx = 0;
      userAnswers = [];
      state = "quiz";
    }

    function render() {
      const content = document.getElementById("content");
      const title = document.getElementById("card-title");
      const progressContainer = document.getElementById("progress-container");

      if (state === "quiz") {
        const q = QUESTION_BANK[currentIdx];
        title.innerText = "Scenario " + (currentIdx + 1);
        progressContainer.style.display = "block";

        const progressPercent = ((currentIdx) / QUESTION_BANK.length) * 100;
        document.getElementById("progress-fill").style.width = progressPercent + "%";
        document.getElementById("progress-text").innerText = `Question ${currentIdx + 1} of ${QUESTION_BANK.length}`;

        let optionsHtml = q.options.map(opt => `
          <div class="option" onclick="handleSelect('${opt.id}')">${opt.text}</div>
        `).join("");

        content.innerHTML = `
          <div class="scenario">${q.scenario}</div>
          <div class="options">${optionsHtml}</div>
        `;
      } else {
        renderResults();
      }
    }

    window.handleSelect = function(optionId) {
      const q = QUESTION_BANK[currentIdx];
      const selected = q.options.find(o => o.id === optionId);
      userAnswers.push({ qId: q.id, selected });

      if (currentIdx < QUESTION_BANK.length - 1) {
        currentIdx++;
        render();
      } else {
        state = "results";
        render();
      }
    };

    function renderResults() {
      const title = document.getElementById("card-title");
      const progressContainer = document.getElementById("progress-container");
      const content = document.getElementById("content");

      title.innerText = "Your Moral Alignment";
      progressContainer.style.display = "none";

      const scores = {};
      const flagsFound = new Set();

      userAnswers.forEach(ans => {
        const w = ans.selected.weights;
        for (let key in w) {
          scores[key] = (scores[key] || 0) + w[key];
        }
        (ans.selected.flags || []).forEach(f => flagsFound.add(f));
      });

      const totalWeight = Object.values(scores).reduce((a, b) => a + b, 0) || 1;
      const frameworkLabels = {
        care: "Ethics of Care",
        util_act: "Act Utilitarianism",
        rights: "Rights-Based Ethics",
        virtue: "Virtue Ethics",
        relativism: "Moral Relativism",
        deon: "Deontological Ethics",
        ie: "Imposition Ethics",
        christian: "Christian Ethics",
        islam: "Islamic Ethics",
        judaism: "Jewish Ethics",
        contract: "Social Contract Theory",
        egoism: "Ethical Egoism",
        confucian: "Confucianism",
        daoist: "Daoism",
        nihilism: "Moral Nihilism"
      };

      const sorted = Object.entries(scores)
        .map(([key, val]) => ({ label: frameworkLabels[key] || key, value: Math.round((val / totalWeight) * 100) }))
        .sort((a, b) => b.value - a.value);

      const top = sorted[0];
      const barsHtml = sorted.map(s => `
        <div class="bar-row">
          <div class="bar-label">${s.label}</div>
          <div class="bar-track"><div class="bar-fill" style="width: ${s.value}%"></div></div>
          <div class="bar-value">${s.value}%</div>
        </div>
      `).join("");

      const tensions = [];
      if (flagsFound.has("outcome_priority") && flagsFound.has("imposition_wrong")) {
        tensions.push("You value outcomes but also reject involuntary imposition.");
      }
      if (flagsFound.has("rules_absolute") && flagsFound.has("moral_permissive")) {
        tensions.push("You respect absolute rules but also see some choices as purely personal.");
      }

      const tensionsHtml = tensions.length > 0 
        ? tensions.map(t => `<div class="tension-item">• ${t}</div>`).join("")
        : `<div class="tension-item" style="color: var(--muted);">No major tensions detected.</div>`;

      content.innerHTML = `
        <div class="results-section-title">Framework breakdown</div>
        <div style="margin-top:6px;">
          ${barsHtml}
        </div>

        <div class="results-section-title">Tensions in your answers</div>
        <div style="margin-top:4px;">
          ${tensionsHtml}
        </div>

        <div class="results-section-title">A possible direction to explore</div>
        <div class="invite">
          Some people who answer like this find it interesting to explore moral frameworks that treat involuntary
          imposition itself as morally significant. If that idea resonates with you, you may enjoy learning more.
        </div>

        <div class="buttons" style="margin-top:16px;">
          <button class="btn-secondary" id="restart-btn">Restart Quiz</button>
        </div>
        <div class="small">
          This is not a diagnosis or a verdict. It is a structured reflection of how you answered these particular scenarios.
        </div>
      `;

      document.getElementById("restart-btn").addEventListener("click", () => {
        resetState();
        render();
      });

      try {
        const payload = {
          type: "MORALITY_RESULT",
          topFramework: top ? top.label : null,
          topPercent: top ? top.value : null,
          tensionCount: tensions.length
        };
        window.parent.postMessage(payload, "*");
      } catch (e) {}
    }

    render();
  </script>
</body>
</html>
